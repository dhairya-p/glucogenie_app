{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Testing & Fine-tuning Notebook\n",
    "\n",
    "This notebook allows you to test the complete RAG pipeline:\n",
    "1. PDF Extraction\n",
    "2. Chunking (namespace-specific strategies)\n",
    "3. Pinecone Ingestion\n",
    "4. RAG Search & Retrieval\n",
    "5. Agent Routing\n",
    "6. End-to-End Chat with Citations\n",
    "\n",
    "**Setup**: Run from `backend/` directory:\n",
    "```bash\n",
    "cd backend\n",
    "jupyter notebook notebooks/test_rag_pipeline.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_core in /opt/anaconda3/lib/python3.9/site-packages (0.3.80)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_core) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_core) (6.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_core) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_core) (2.12.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.5.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.3)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.26.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pinecone in /opt/anaconda3/lib/python3.9/site-packages (7.3.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone) (2022.9.24)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone) (1.8.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone) (4.15.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone) (1.26.11)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /opt/anaconda3/lib/python3.9/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.9/site-packages (from dotenv) (1.2.1)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Installing collected packages: dotenv\n",
      "Successfully installed dotenv-0.9.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain_openai in /opt/anaconda3/lib/python3.9/site-packages (0.3.35)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.78 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_openai) (0.3.80)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_openai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain_openai) (0.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (6.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.12.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.5.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.3)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.64.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.9/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2022.7.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.26.11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting supabase\n",
      "  Using cached supabase-2.25.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting realtime==2.25.1 (from supabase)\n",
      "  Using cached realtime-2.25.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting supabase-functions==2.25.1 (from supabase)\n",
      "  Using cached supabase_functions-2.25.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting storage3==2.25.1 (from supabase)\n",
      "  Using cached storage3-2.25.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting supabase-auth==2.25.1 (from supabase)\n",
      "  Using cached supabase_auth-2.25.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting postgrest==2.25.1 (from supabase)\n",
      "  Using cached postgrest-2.25.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.26 in /opt/anaconda3/lib/python3.9/site-packages (from supabase) (0.28.1)\n",
      "Collecting deprecation>=2.1.0 (from postgrest==2.25.1->supabase)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.9 in /opt/anaconda3/lib/python3.9/site-packages (from postgrest==2.25.1->supabase) (2.12.5)\n",
      "Collecting strenum>=0.4.9 (from postgrest==2.25.1->supabase)\n",
      "  Using cached StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting yarl>=1.20.1 (from postgrest==2.25.1->supabase)\n",
      "  Using cached yarl-1.22.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in /opt/anaconda3/lib/python3.9/site-packages (from realtime==2.25.1->supabase) (4.15.0)\n",
      "Collecting websockets<16,>=11 (from realtime==2.25.1->supabase)\n",
      "  Downloading websockets-15.0.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pyjwt>=2.10.1 (from pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase)\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.9/site-packages (from httpx<0.29,>=0.26->supabase) (3.5.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.9/site-packages (from httpx<0.29,>=0.26->supabase) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.9/site-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.9/site-packages (from httpx<0.29,>=0.26->supabase) (3.3)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]<0.29,>=0.26->postgrest==2.25.1->supabase)\n",
      "  Using cached h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.25.1->supabase)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.25.1->supabase)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0,>=1.9->postgrest==2.25.1->supabase) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0,>=1.9->postgrest==2.25.1->supabase) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0,>=1.9->postgrest==2.25.1->supabase) (0.4.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.9/site-packages (from deprecation>=2.1.0->postgrest==2.25.1->supabase) (24.2)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase) (37.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.9/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase) (2.21)\n",
      "Collecting multidict>=4.0 (from yarl>=1.20.1->postgrest==2.25.1->supabase)\n",
      "  Using cached multidict-6.7.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.1 (from yarl>=1.20.1->postgrest==2.25.1->supabase)\n",
      "  Using cached propcache-0.4.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.2.0)\n",
      "Downloading supabase-2.25.1-py3-none-any.whl (16 kB)\n",
      "Downloading postgrest-2.25.1-py3-none-any.whl (21 kB)\n",
      "Downloading realtime-2.25.1-py3-none-any.whl (22 kB)\n",
      "Downloading storage3-2.25.1-py3-none-any.whl (26 kB)\n",
      "Downloading supabase_auth-2.25.1-py3-none-any.whl (48 kB)\n",
      "Downloading supabase_functions-2.25.1-py3-none-any.whl (8.5 kB)\n",
      "Using cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading websockets-15.0.1-cp39-cp39-macosx_10_9_x86_64.whl (173 kB)\n",
      "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Using cached yarl-1.22.0-cp39-cp39-macosx_10_9_x86_64.whl (93 kB)\n",
      "Using cached multidict-6.7.0-cp39-cp39-macosx_10_9_x86_64.whl (44 kB)\n",
      "Using cached propcache-0.4.1-cp39-cp39-macosx_10_9_x86_64.whl (45 kB)\n",
      "Installing collected packages: strenum, websockets, pyjwt, propcache, multidict, hyperframe, hpack, deprecation, yarl, h2, realtime, supabase-functions, supabase-auth, storage3, postgrest, supabase\n",
      "\u001b[2K  Attempting uninstall: pyjwt\n",
      "\u001b[2K    Found existing installation: PyJWT 2.4.0\n",
      "\u001b[2K    Uninstalling PyJWT-2.4.0:\n",
      "\u001b[2K      Successfully uninstalled PyJWT-2.4.0\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [supabase]━━━━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [h2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed deprecation-2.1.0 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 multidict-6.7.0 postgrest-2.25.1 propcache-0.4.1 pyjwt-2.10.1 realtime-2.25.1 storage3-2.25.1 strenum-0.4.15 supabase-2.25.1 supabase-auth-2.25.1 supabase-functions-2.25.1 websockets-15.0.1 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_core\n",
    "%pip install pinecone\n",
    "%pip install dotenv\n",
    "%pip install langchain_openai\n",
    "%pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_client' from 'supabase' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0m/rb0qcj9s5v32nxlmn24w42880000gn/T/ipykernel_19084/3911884499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msupabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msupabase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ NumPy Version: {numpy.__version__} (Should be >= 1.23)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_client' from 'supabase' (unknown location)"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import supabase\n",
    "from supabase import create_client, Client\n",
    "\n",
    "print(f\"✅ NumPy Version: {numpy.__version__} (Should be >= 1.23)\")\n",
    "print(f\"✅ Supabase Client found successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: /Users/dhairyapatel/Desktop/FYP/diabetes_fyp/backend\n"
     ]
    }
   ],
   "source": [
    "# Add backend to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "backend_dir = project_root / 'backend'\n",
    "\n",
    "# Add to sys.path if it's not already there\n",
    "if str(backend_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(backend_dir))\n",
    "\n",
    "print(f\"Added to path: {backend_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check:\n",
      "✓ OPENAI_API_KEY: Set\n",
      "✓ PINECONE_API_KEY: Set\n",
      "✓ PINECONE_INDEX: diabetes-medical-knowledge\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(backend_dir / \".env\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"Environment Check:\")\n",
    "print(f\"✓ OPENAI_API_KEY: {'Set' if os.getenv('OPENAI_API_KEY') else '❌ Missing'}\")\n",
    "print(f\"✓ PINECONE_API_KEY: {'Set' if os.getenv('PINECONE_API_KEY') else '❌ Missing'}\")\n",
    "print(f\"✓ PINECONE_INDEX: {os.getenv('PINECONE_INDEX', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Client' from 'supabase' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0m/rb0qcj9s5v32nxlmn24w42880000gn/T/ipykernel_19084/3168523935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouter_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouterState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute_intent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclinical_safety_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClinicalSafetyState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_clinical_safety\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifestyle_analyst_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLifestyleState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyze_lifestyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatient_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPatientContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhanced_patient_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnhancedPatientContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/diabetes_fyp/backend/app/agents/lifestyle_analyst_agent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatient_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPatientContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhanced_patient_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnhancedPatientContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupabase_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_supabase_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m from app.core.timezone_utils import (\n\u001b[1;32m     17\u001b[0m     \u001b[0mget_singapore_now\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/diabetes_fyp/backend/app/core/supabase_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlru_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msupabase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Client' from 'supabase' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Import RAG services and agents\n",
    "from app.services.rag_service import (\n",
    "    get_rag_service,\n",
    "    NAMESPACE_CLINICAL_SAFETY,\n",
    "    NAMESPACE_CULTURAL_DIET,\n",
    "    NAMESPACE_LIFESTYLE_PATTERNS,\n",
    ")\n",
    "from app.agents.router_agent import RouterState, route_intent\n",
    "from app.agents.clinical_safety_agent import ClinicalSafetyState, check_clinical_safety\n",
    "from app.agents.lifestyle_analyst_agent import LifestyleState, analyze_lifestyle\n",
    "from app.schemas.patient_context import PatientContext\n",
    "from app.schemas.enhanced_patient_context import EnhancedPatientContext\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Extraction Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PDF extraction utilities\n",
    "try:\n",
    "    import pdfplumber\n",
    "    PDFPLUMBER_AVAILABLE = True\n",
    "    print(\"✓ pdfplumber available\")\n",
    "except ImportError:\n",
    "    PDFPLUMBER_AVAILABLE = False\n",
    "    print(\"❌ pdfplumber not available\")\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "    PYMUPDF_AVAILABLE = True\n",
    "    print(\"✓ PyMuPDF available\")\n",
    "except ImportError:\n",
    "    PYMUPDF_AVAILABLE = False\n",
    "    print(\"❌ PyMuPDF not available\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract text from PDF file.\"\"\"\n",
    "    text_content = []\n",
    "    \n",
    "    if PDFPLUMBER_AVAILABLE:\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        text_content.append(f\"Page {page_num}:\\n{text}\\n\")\n",
    "            print(f\"✓ Extracted {len(text_content)} pages using pdfplumber\")\n",
    "            return \"\\n\".join(text_content)\n",
    "        except Exception as e:\n",
    "            print(f\"pdfplumber failed: {e}\")\n",
    "    \n",
    "    if PYMUPDF_AVAILABLE:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page_num, page in enumerate(doc, 1):\n",
    "                text = page.get_text()\n",
    "                if text:\n",
    "                    text_content.append(f\"Page {page_num}:\\n{text}\\n\")\n",
    "            doc.close()\n",
    "            print(f\"✓ Extracted {len(text_content)} pages using PyMuPDF\")\n",
    "            return \"\\n\".join(text_content)\n",
    "        except Exception as e:\n",
    "            print(f\"PyMuPDF failed: {e}\")\n",
    "    \n",
    "    raise RuntimeError(\"Could not extract text. Install pdfplumber or pymupdf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PDF extraction on a sample file\n",
    "rag_docs_dir = backend_dir / \"rag_docs\"\n",
    "clinical_docs_dir = rag_docs_dir / \"clinical_safety_docs\"\n",
    "\n",
    "# List available PDFs\n",
    "if clinical_docs_dir.exists():\n",
    "    pdf_files = list(clinical_docs_dir.glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDFs in clinical_safety_docs/\")\n",
    "    for pdf in pdf_files[:5]:  # Show first 5\n",
    "        print(f\"  - {pdf.name} ({pdf.stat().st_size / 1024:.0f} KB)\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        # Test extraction on first small PDF\n",
    "        test_pdf = min(pdf_files, key=lambda p: p.stat().st_size)  # Smallest file\n",
    "        print(f\"\\nTesting extraction on: {test_pdf.name}\")\n",
    "        \n",
    "        extracted_text = extract_text_from_pdf(test_pdf)\n",
    "        print(f\"\\nExtracted {len(extracted_text)} characters\")\n",
    "        print(\"\\nFirst 500 characters:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(extracted_text[:500])\n",
    "        print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"❌ rag_docs/clinical_safety_docs/ directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunking Strategy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import chunking functions\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def chunk_with_headers(text: str, source: str, min_chunk_size: int = 200, max_chunk_size: int = 1000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Chunk text preserving headers (for clinical safety documents).\"\"\"\n",
    "    chunks = []\n",
    "    lines = text.split('\\n')\n",
    "    current_header_stack = []\n",
    "    current_chunk = []\n",
    "    current_chunk_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        if not line_stripped:\n",
    "            continue\n",
    "        \n",
    "        # Detect headers\n",
    "        is_header = (\n",
    "            line_stripped.isupper() and len(line_stripped) > 3 and len(line_stripped) < 100\n",
    "            or re.match(r'^\\d+\\.?\\s+[A-Z]', line_stripped)\n",
    "            or re.match(r'^[A-Z][A-Z\\s]{3,}', line_stripped)\n",
    "            or line_stripped.endswith(':') and len(line_stripped) < 80\n",
    "        )\n",
    "        \n",
    "        if is_header:\n",
    "            if current_chunk_size >= min_chunk_size:\n",
    "                header_path = \" > \".join(current_header_stack) if current_header_stack else \"Introduction\"\n",
    "                chunk_text = \"\\n\".join(current_chunk)\n",
    "                chunks.append({\n",
    "                    \"content\": f\"Header: {header_path}\\n{chunk_text}\",\n",
    "                    \"metadata\": {\n",
    "                        \"source\": source,\n",
    "                        \"header_path\": header_path,\n",
    "                        \"tags\": [h.lower().replace(\" \", \"_\") for h in current_header_stack],\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            if len(current_header_stack) < 3:\n",
    "                current_header_stack.append(line_stripped)\n",
    "            else:\n",
    "                current_header_stack[-1] = line_stripped\n",
    "            \n",
    "            current_chunk = [line_stripped]\n",
    "            current_chunk_size = len(line_stripped)\n",
    "        else:\n",
    "            current_chunk.append(line_stripped)\n",
    "            current_chunk_size += len(line_stripped) + 1\n",
    "            \n",
    "            if current_chunk_size >= max_chunk_size:\n",
    "                header_path = \" > \".join(current_header_stack) if current_header_stack else \"Introduction\"\n",
    "                chunk_text = \"\\n\".join(current_chunk)\n",
    "                chunks.append({\n",
    "                    \"content\": f\"Header: {header_path}\\n{chunk_text}\",\n",
    "                    \"metadata\": {\n",
    "                        \"source\": source,\n",
    "                        \"header_path\": header_path,\n",
    "                        \"tags\": [h.lower().replace(\" \", \"_\") for h in current_header_stack],\n",
    "                    }\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_chunk_size = 0\n",
    "    \n",
    "    if current_chunk_size >= min_chunk_size:\n",
    "        header_path = \" > \".join(current_header_stack) if current_header_stack else \"Introduction\"\n",
    "        chunk_text = \"\\n\".join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"content\": f\"Header: {header_path}\\n{chunk_text}\",\n",
    "            \"metadata\": {\n",
    "                \"source\": source,\n",
    "                \"header_path\": header_path,\n",
    "                \"tags\": [h.lower().replace(\" \", \"_\") for h in current_header_stack],\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"✓ Chunking function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunking on extracted text\n",
    "if 'extracted_text' in locals():\n",
    "    chunks = chunk_with_headers(extracted_text, source=test_pdf.name)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks from {len(extracted_text)} characters\")\n",
    "    print(f\"Average chunk size: {sum(len(c['content']) for c in chunks) / len(chunks):.0f} chars\")\n",
    "    \n",
    "    # Show first 3 chunks\n",
    "    print(\"\\nFirst 3 chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:3], 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Chunk {i}:\")\n",
    "        print(f\"Source: {chunk['metadata']['source']}\")\n",
    "        print(f\"Header: {chunk['metadata']['header_path']}\")\n",
    "        print(f\"Tags: {chunk['metadata'].get('tags', [])}\")\n",
    "        print(f\"Length: {len(chunk['content'])} chars\")\n",
    "        print(f\"\\nContent preview (first 300 chars):\")\n",
    "        print(chunk['content'][:300])\n",
    "else:\n",
    "    print(\"No extracted text available. Run PDF extraction cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pinecone Connection & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG service\n",
    "rag = get_rag_service()\n",
    "\n",
    "if rag.is_available():\n",
    "    print(\"✓ RAG service initialized successfully\")\n",
    "    print(f\"  Index: {rag.index_name}\")\n",
    "    \n",
    "    # Get index stats\n",
    "    try:\n",
    "        stats = rag.index.describe_index_stats()\n",
    "        print(f\"\\nIndex Statistics:\")\n",
    "        print(f\"  Total vectors: {stats.get('total_vector_count', 'N/A')}\")\n",
    "        print(f\"  Dimensions: {stats.get('dimension', 'N/A')}\")\n",
    "        \n",
    "        namespaces = stats.get('namespaces', {})\n",
    "        if namespaces:\n",
    "            print(f\"\\n  Namespaces:\")\n",
    "            for ns, ns_stats in namespaces.items():\n",
    "                print(f\"    - {ns}: {ns_stats.get('vector_count', 0)} vectors\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch index stats: {e}\")\n",
    "else:\n",
    "    print(\"❌ RAG service not available. Check PINECONE_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Search & Retrieval Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG search on different namespaces\n",
    "if rag.is_available():\n",
    "    test_queries = [\n",
    "        {\n",
    "            \"query\": \"metformin kidney disease contraindication\",\n",
    "            \"namespace\": NAMESPACE_CLINICAL_SAFETY,\n",
    "            \"top_k\": 3\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"glucose target Type 2 Diabetes elderly\",\n",
    "            \"namespace\": NAMESPACE_LIFESTYLE_PATTERNS,\n",
    "            \"top_k\": 3\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Hainanese Chicken Rice nutritional carbs\",\n",
    "            \"namespace\": NAMESPACE_CULTURAL_DIET,\n",
    "            \"top_k\": 2\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for test in test_queries:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Query: {test['query']}\")\n",
    "        print(f\"Namespace: {test['namespace']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        results = rag.search(test['query'], namespace=test['namespace'], top_k=test['top_k'])\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\nFound {len(results)} results:\")\n",
    "            for i, result in enumerate(results, 1):\n",
    "                print(f\"\\nResult {i}:\")\n",
    "                print(f\"  Score: {result.get('score', 'N/A')}\")\n",
    "                print(f\"  Source: {result.get('metadata', {}).get('source', 'Unknown')}\")\n",
    "                print(f\"  Tags: {result.get('metadata', {}).get('tags', [])}\")\n",
    "                print(f\"  Content preview: {result.get('content', '')[:200]}...\")\n",
    "        else:\n",
    "            print(\"❌ No results found\")\n",
    "else:\n",
    "    print(\"RAG service not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test formatted context for LLM (with citations)\n",
    "if rag.is_available():\n",
    "    query = \"MOH guidelines for metformin use in elderly patients\"\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Namespace: {NAMESPACE_CLINICAL_SAFETY}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    context = rag.get_context_for_llm(\n",
    "        query=query,\n",
    "        namespace=NAMESPACE_CLINICAL_SAFETY,\n",
    "        top_k=3,\n",
    "        include_citations=True\n",
    "    )\n",
    "    \n",
    "    if context:\n",
    "        print(\"\\nFormatted Context (with mandatory citations):\")\n",
    "        print(\"=\"*80)\n",
    "        print(context)\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Check for citation markers\n",
    "        source_count = context.count(\"Source:\")\n",
    "        citation_instruction_count = context.count(\"you MUST cite\")\n",
    "        print(f\"\\nCitation Analysis:\")\n",
    "        print(f\"  Sources mentioned: {source_count}\")\n",
    "        print(f\"  Citation instructions: {citation_instruction_count}\")\n",
    "    else:\n",
    "        print(\"❌ No context returned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent Routing Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test patient context\n",
    "test_patient = PatientContext(\n",
    "    user_id=\"test-user-123\",\n",
    "    first_name=\"John\",\n",
    "    last_name=\"Doe\",\n",
    "    age=65,\n",
    "    sex=\"Male\",\n",
    "    ethnicity=\"Chinese\",\n",
    "    height=170.0,\n",
    "    activity_level=\"moderate\",\n",
    "    location=\"Singapore\",\n",
    "    conditions=[\"Type 2 Diabetes\", \"Hypertension\"],\n",
    "    medications=[\"Metformin 500mg\", \"Lisinopril 10mg\"]\n",
    ")\n",
    "\n",
    "print(\"Test patient created:\")\n",
    "print(f\"  Name: {test_patient.first_name} {test_patient.last_name}\")\n",
    "print(f\"  Age: {test_patient.age}\")\n",
    "print(f\"  Conditions: {test_patient.conditions}\")\n",
    "print(f\"  Medications: {test_patient.medications}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test router agent with different queries\n",
    "test_routing_queries = [\n",
    "    \"What are the side effects of metformin?\",  # Should route to clinical_safety\n",
    "    \"What do MOH guidelines say about diabetes management?\",  # Should route to clinical_safety\n",
    "    \"What was my average glucose this week?\",  # Should route to lifestyle_analyst\n",
    "    \"Did I take my medication today?\",  # Should route to lifestyle_analyst\n",
    "    \"Hello, how are you?\",  # Should route to unmatched\n",
    "]\n",
    "\n",
    "for query in test_routing_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    state = RouterState(patient=test_patient, user_message=query)\n",
    "    decision = route_intent.invoke(input=state)\n",
    "    \n",
    "    print(f\"\\nRouting Decision:\")\n",
    "    print(f\"  Target Agent: {decision['target_agent']}\")\n",
    "    print(f\"  Intent: {decision['intent']}\")\n",
    "    print(f\"  Rationale: {decision['rationale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clinical Safety Agent Testing (with RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Clinical Safety Agent with RAG integration\n",
    "test_clinical_query = \"What do MOH guidelines say about metformin use in elderly patients with kidney disease?\"\n",
    "\n",
    "print(f\"Query: {test_clinical_query}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create agent state\n",
    "clinical_state = ClinicalSafetyState(\n",
    "    patient=test_patient,\n",
    "    user_message=test_clinical_query,\n",
    "    enhanced_context=None  # Can add enhanced context if needed\n",
    ")\n",
    "\n",
    "# Invoke agent\n",
    "result = check_clinical_safety.invoke(input=clinical_state)\n",
    "\n",
    "print(\"\\nClinical Safety Analysis:\")\n",
    "print(f\"  Is Safe: {result.get('is_safe', 'N/A')}\")\n",
    "print(f\"  Warnings ({len(result.get('warnings', []))}):\")\n",
    "for warning in result.get('warnings', []):\n",
    "    print(f\"    - {warning}\")\n",
    "print(f\"\\n  Rationale: {result.get('rationale', 'N/A')}\")\n",
    "\n",
    "# Check RAG context\n",
    "rag_context = result.get('rag_context', '')\n",
    "rag_citations = result.get('rag_citations', [])\n",
    "\n",
    "print(f\"\\n  RAG Context Length: {len(rag_context)} chars\")\n",
    "print(f\"  RAG Citations ({len(rag_citations)}): {rag_citations}\")\n",
    "\n",
    "if rag_context:\n",
    "    print(\"\\n  RAG Context Preview (first 500 chars):\")\n",
    "    print(\"  \" + \"-\"*78)\n",
    "    print(\"  \" + rag_context[:500])\n",
    "    print(\"  \" + \"-\"*78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. End-to-End Chat Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate full chat flow: Router → Agent → LLM Response\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from app.core.system_prompt_builder import build_system_prompt\n",
    "\n",
    "def test_full_chat_flow(user_query: str, patient: PatientContext):\n",
    "    \"\"\"Test complete chat flow with RAG citations.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"User Query: {user_query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Step 1: Router\n",
    "    router_state = RouterState(patient=patient, user_message=user_query)\n",
    "    routing_decision = route_intent.invoke(input=router_state)\n",
    "    print(f\"\\n[ROUTER] Target Agent: {routing_decision['target_agent']}\")\n",
    "    \n",
    "    # Step 2: Agent (Clinical Safety)\n",
    "    if routing_decision['target_agent'] == 'clinical_safety':\n",
    "        clinical_state = ClinicalSafetyState(\n",
    "            patient=patient,\n",
    "            user_message=user_query,\n",
    "            enhanced_context=None\n",
    "        )\n",
    "        agent_result = check_clinical_safety.invoke(input=clinical_state)\n",
    "        \n",
    "        rag_context = agent_result.get('rag_context', '')\n",
    "        rag_citations = agent_result.get('rag_citations', [])\n",
    "        agent_text = agent_result.get('rationale', '')\n",
    "        \n",
    "        print(f\"[AGENT] RAG Citations: {rag_citations}\")\n",
    "        print(f\"[AGENT] RAG Context Length: {len(rag_context)} chars\")\n",
    "        \n",
    "        # Step 3: Build System Prompt\n",
    "        patient_str = f\"{patient.first_name} {patient.last_name}, {patient.age}yo {patient.sex}\"\n",
    "        system_prompt = build_system_prompt(\n",
    "            patient_context_str=patient_str,\n",
    "            enhanced_context=None,\n",
    "            user_message=user_query,\n",
    "            agent_text=agent_text,\n",
    "            rag_context=rag_context\n",
    "        )\n",
    "        \n",
    "        # Step 4: LLM Call\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_query)\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n[LLM] Calling GPT-4o-mini...\")\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"LLM RESPONSE:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.content)\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Step 5: Citation Validation\n",
    "        print(f\"\\n[VALIDATION] Citation Check:\")\n",
    "        citations_found = []\n",
    "        for citation in rag_citations:\n",
    "            clean_citation = citation.replace('.pdf', '').replace('_', ' ')\n",
    "            if clean_citation in response.content or citation in response.content:\n",
    "                citations_found.append(citation)\n",
    "                print(f\"  ✓ Found citation: {citation}\")\n",
    "            else:\n",
    "                print(f\"  ❌ Missing citation: {citation}\")\n",
    "        \n",
    "        if citations_found:\n",
    "            print(f\"\\n  ✓ SUCCESS: {len(citations_found)}/{len(rag_citations)} citations included\")\n",
    "        else:\n",
    "            print(f\"\\n  ❌ FAILURE: No citations found in response\")\n",
    "        \n",
    "        return response.content, citations_found\n",
    "    else:\n",
    "        print(f\"[INFO] Not a clinical safety query, skipping detailed flow\")\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test full chat flow with citation validation\n",
    "test_queries = [\n",
    "    \"What do MOH guidelines recommend for metformin dosing in elderly patients?\",\n",
    "    \"Are there any contraindications for metformin in patients with kidney disease?\",\n",
    "    \"What are the ADA recommendations for HbA1c targets?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    response, citations = test_full_chat_flow(query, test_patient)\n",
    "    print(\"\\n\" + \"#\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fine-tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different RAG parameters\n",
    "print(\"RAG Parameter Tuning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if rag.is_available():\n",
    "    query = \"metformin kidney disease contraindication\"\n",
    "    \n",
    "    # Test different top_k values\n",
    "    for top_k in [1, 3, 5]:\n",
    "        results = rag.search(query, namespace=NAMESPACE_CLINICAL_SAFETY, top_k=top_k)\n",
    "        print(f\"\\ntop_k={top_k}: {len(results)} results\")\n",
    "        if results:\n",
    "            avg_score = sum(r.get('score', 0) for r in results) / len(results)\n",
    "            print(f\"  Average score: {avg_score:.4f}\")\n",
    "            print(f\"  Min score: {min(r.get('score', 0) for r in results):.4f}\")\n",
    "            print(f\"  Max score: {max(r.get('score', 0) for r in results):.4f}\")\n",
    "else:\n",
    "    print(\"RAG service not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PIPELINE TESTING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Components Tested:\")\n",
    "print(\"  1. PDF Extraction (pdfplumber/PyMuPDF)\")\n",
    "print(\"  2. Chunking Strategy (header-based for clinical docs)\")\n",
    "print(\"  3. Pinecone Connection & Stats\")\n",
    "print(\"  4. RAG Search & Retrieval\")\n",
    "print(\"  5. Agent Routing (LLM-based)\")\n",
    "print(\"  6. Clinical Safety Agent with RAG\")\n",
    "print(\"  7. End-to-End Chat with Citation Validation\")\n",
    "print(\"\\n⚠️ Known Issues:\")\n",
    "print(\"  - Citations may not appear consistently in LLM responses\")\n",
    "print(\"  - LLM relies on instructions, no programmatic enforcement\")\n",
    "print(\"  - rag_citations array collected but not fully utilized\")\n",
    "print(\"\\n💡 Recommendations:\")\n",
    "print(\"  1. Add post-processing to validate citations in responses\")\n",
    "print(\"  2. Simplify citation instructions (fewer lines, clearer format)\")\n",
    "print(\"  3. Consider using JSON mode for structured output with citations\")\n",
    "print(\"  4. Monitor RAG retrieval scores (threshold: >0.7 for relevance)\")\n",
    "print(\"  5. Fine-tune chunk sizes based on retrieval quality\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
