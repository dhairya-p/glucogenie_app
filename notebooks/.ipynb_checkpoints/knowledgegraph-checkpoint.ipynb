{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49bd833-611b-4e3f-834d-394d1ccc9bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.14.13-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting llama-index-graph-stores-neo4j\n",
      "  Downloading llama_index_graph_stores_neo4j-0.5.1-py3-none-any.whl.metadata (406 bytes)\n",
      "Collecting llama-index-vector-stores-pinecone\n",
      "  Using cached llama_index_vector_stores_pinecone-0.7.1-py3-none-any.whl.metadata (424 bytes)\n",
      "Collecting llama-index-llms-openai\n",
      "  Downloading llama_index_llms_openai-0.6.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)\n",
      "  Using cached llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.15.0,>=0.14.13 (from llama-index)\n",
      "  Downloading llama_index_core-0.14.13-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.5.6-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index) (3.9.2)\n",
      "Requirement already satisfied: openai<3,>=1.108.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-llms-openai) (2.14.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (3.13.2)\n",
      "Collecting aiosqlite (from llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached aiosqlite-0.22.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Downloading banks-2.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.6.7)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2025.10.0)\n",
      "Requirement already satisfied: httpx in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.28.1)\n",
      "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Downloading llama_index_workflows-2.13.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (3.5)\n",
      "Requirement already satisfied: numpy in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2.3.5)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (12.0.0)\n",
      "Requirement already satisfied: platformdirs in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2.12.4)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.13->llama-index) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.13->llama-index) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.22.0)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.1.6)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pandas<3,>=2.0.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.3.3)\n",
      "Collecting pypdf<7,>=6.1.3 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
      "  Downloading pypdf-6.6.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.5)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai<3,>=1.108.1->llama-index-llms-openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.13->llama-index) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.13->llama-index) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (0.4.2)\n",
      "Collecting neo4j<6,>=5.16.0 (from llama-index-graph-stores-neo4j)\n",
      "  Downloading neo4j-5.28.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pinecone<8.0.0,>=7.0.0 (from llama-index-vector-stores-pinecone)\n",
      "  Using cached pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone<8.0.0,>=7.0.0->llama-index-vector-stores-pinecone)\n",
      "  Using cached pinecone_plugin_assistant-1.8.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pinecone<8.0.0,>=7.0.0->llama-index-vector-stores-pinecone) (0.0.7)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pinecone<8.0.0,>=7.0.0->llama-index-vector-stores-pinecone) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone<8.0.0,>=7.0.0->llama-index-vector-stores-pinecone) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.4.4)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.92-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.92 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.92-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: click<9,>=8.1.7 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-cloud-services>=0.6.92->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.2.1)\n",
      "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.91-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.91 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.91-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.90-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.90 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.90-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.89-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-cloud-services>=0.6.89 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.89-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Using cached llama_parse-0.6.88-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting llama-cloud-services>=0.6.88 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Using cached llama_cloud_services-0.6.88-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.87-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.87 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.87-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.86-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.86 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.86-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.85-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.85 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.85-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.84-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.84 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.84-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.83-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.82 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.83-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_cloud_services-0.6.82-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.82-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading llama_parse-0.6.81-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.81 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.81-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.80-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.80 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.80-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.79-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.79 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.79-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.78-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.78 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.78-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.77-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.77 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.77-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
      "Requirement already satisfied: joblib in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from nltk>3.8.1->llama-index) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from nltk>3.8.1->llama-index) (2025.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.26.2)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.13->llama-index)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dhairyapatel/opt/anaconda3/lib/python3.13/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.13->llama-index) (3.0.2)\n",
      "Downloading llama_index-0.14.13-py3-none-any.whl (7.5 kB)\n",
      "Downloading llama_index_llms_openai-0.6.16-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.14.13-py3-none-any.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.3.0-py3-none-any.whl (32 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
      "Downloading llama_index_readers_file-0.5.6-py3-none-any.whl (51 kB)\n",
      "Downloading llama_index_workflows-2.13.1-py3-none-any.whl (107 kB)\n",
      "Downloading pypdf-6.6.2-py3-none-any.whl (329 kB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_index_graph_stores_neo4j-0.5.1-py3-none-any.whl (18 kB)\n",
      "Downloading neo4j-5.28.3-py3-none-any.whl (313 kB)\n",
      "Downloading llama_index_vector_stores_pinecone-0.7.1-py3-none-any.whl (8.0 kB)\n",
      "Using cached pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
      "Using cached pinecone_plugin_assistant-1.8.0-py3-none-any.whl (259 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
      "Using cached llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
      "Downloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
      "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
      "Using cached aiosqlite-0.22.1-py3-none-any.whl (17 kB)\n",
      "Using cached griffe-1.15.0-py3-none-any.whl (150 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, pypdf, neo4j, deprecated, colorama, aiosqlite, pinecone-plugin-assistant, griffe, pinecone, llama-index-instrumentation, llama-cloud, banks, llama-index-workflows, llama-index-core, llama-index-vector-stores-pinecone, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-graph-stores-neo4j, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-cli, llama-index-readers-llama-parse, llama-index\n",
      "\u001b[2K  Attempting uninstall: neo4j0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/27\u001b[0m [pypdf]\n",
      "\u001b[2K    Found existing installation: neo4j 6.0.3━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/27\u001b[0m [pypdf]\n",
      "\u001b[2K    Uninstalling neo4j-6.0.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/27\u001b[0m [pypdf]\n",
      "\u001b[2K      Successfully uninstalled neo4j-6.0.3━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/27\u001b[0m [pypdf]\n",
      "\u001b[2K  Attempting uninstall: pinecone-plugin-assistant━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/27\u001b[0m [deprecated]\n",
      "\u001b[2K    Found existing installation: pinecone-plugin-assistant 3.0.10m \u001b[32m 5/27\u001b[0m [deprecated]\n",
      "\u001b[2K    Uninstalling pinecone-plugin-assistant-3.0.1:━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/27\u001b[0m [deprecated]\n",
      "\u001b[2K      Successfully uninstalled pinecone-plugin-assistant-3.0.1\u001b[0m \u001b[32m 5/27\u001b[0m [deprecated]\n",
      "\u001b[2K  Attempting uninstall: pinecone[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/27\u001b[0m [griffe]\n",
      "\u001b[2K    Found existing installation: pinecone 8.0.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/27\u001b[0m [griffe]\n",
      "\u001b[2K    Uninstalling pinecone-8.0.0:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/27\u001b[0m [griffe]\n",
      "\u001b[2K      Successfully uninstalled pinecone-8.0.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/27\u001b[0m [griffe]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [llama-index]\u001b[0m [llama-index-core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiosqlite-0.22.1 banks-2.3.0 colorama-0.4.6 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.15.0 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.13 llama-index-cli-0.5.3 llama-index-core-0.14.13 llama-index-embeddings-openai-0.5.1 llama-index-graph-stores-neo4j-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-openai-0.6.16 llama-index-readers-file-0.5.6 llama-index-readers-llama-parse-0.5.1 llama-index-vector-stores-pinecone-0.7.1 llama-index-workflows-2.13.1 llama-parse-0.6.54 neo4j-5.28.3 pinecone-7.3.0 pinecone-plugin-assistant-1.8.0 pypdf-6.6.2 striprtf-0.0.26\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index llama-index-graph-stores-neo4j llama-index-vector-stores-pinecone llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5fb132b-7b03-4fe0-9625-6c6f57971eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import KnowledgeGraphIndex, Settings, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde12d5d-d00a-4a4d-8c70-c216a52c232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook is in /notebooks\n",
    "NOTEBOOKS_DIR = Path(\".\").resolve()\n",
    "\n",
    "# Load from environment (.env one level above notebooks)\n",
    "load_dotenv(Path(\"../.env\"), override=True)\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX = os.getenv(\"PINECONE_INDEX\", \"diabetes-medical-knowledge\")\n",
    "\n",
    "missing = [\n",
    "    name\n",
    "    for name, value in {\n",
    "        \"NEO4J_URI\": NEO4J_URI,\n",
    "        \"NEO4J_USERNAME\": NEO4J_USERNAME,\n",
    "        \"NEO4J_PASSWORD\": NEO4J_PASSWORD,\n",
    "        \"OPENAI_API_KEY\": OPENAI_API_KEY,\n",
    "        \"PINECONE_API_KEY\": PINECONE_API_KEY,\n",
    "    }.items()\n",
    "    if not value\n",
    "]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing env vars: {', '.join(missing)}\")\n",
    "\n",
    "TARGET_DRUGS = [\n",
    "    \"metformin\",\n",
    "    \"insulin\",\n",
    "    \"glipizide\",\n",
    "    \"jardiance\",\n",
    "    \"ozempic\",\n",
    "    \"lantus\",\n",
    "]\n",
    "\n",
    "CLINICAL_DIR = NOTEBOOKS_DIR / \"clinical_safety_docs\"\n",
    "DIET_DIR = NOTEBOOKS_DIR / \"dietician_docs\"\n",
    "INTERACTION_DIR = NOTEBOOKS_DIR / \"drug_interaction_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b607cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded clinical=1, interactions=6, diet=1\n"
     ]
    }
   ],
   "source": [
    "def load_md_docs(directory: Path, tag: str) -> list:\n",
    "    files = sorted(directory.glob(\"*.md\"))\n",
    "    if not files:\n",
    "        print(f\"No files found in {directory}\")\n",
    "        return []\n",
    "\n",
    "    def _metadata(path: str) -> dict:\n",
    "        path_obj = Path(path)\n",
    "        return {\n",
    "            \"source\": path_obj.name,\n",
    "            \"doc_path\": str(path_obj),\n",
    "            \"doc_tag\": tag,\n",
    "        }\n",
    "\n",
    "    return SimpleDirectoryReader(\n",
    "        input_files=[str(p) for p in files],\n",
    "        file_metadata=_metadata,\n",
    "    ).load_data()\n",
    "\n",
    "\n",
    "clinical_docs = load_md_docs(CLINICAL_DIR, \"clinical_safety\")\n",
    "interaction_docs = load_md_docs(INTERACTION_DIR, \"drug_interactions\")\n",
    "diet_docs = load_md_docs(DIET_DIR, \"dietician_docs\")\n",
    "\n",
    "print(\n",
    "    f\"Loaded clinical={len(clinical_docs)}, interactions={len(interaction_docs)}, diet={len(diet_docs)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2bd875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG docs from drug_interaction_docs: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 16:00:45,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:47,147 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:48,760 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:50,214 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:51,880 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:53,690 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:55,984 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:57,438 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:00:58,798 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:00,144 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:01,644 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:03,562 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:05,105 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:06,634 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:08,170 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:09,907 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:11,241 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:13,083 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:14,852 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:16,552 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:18,161 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:19,842 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:21,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:23,529 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:25,171 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:26,533 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:28,049 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:29,751 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:31,254 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:32,494 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:34,282 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:35,816 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:37,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:38,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:40,084 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:41,655 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:43,190 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:44,976 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:46,571 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:48,102 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:49,708 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:51,379 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:52,851 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:54,490 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:56,372 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:57,663 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:01:59,258 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:01,073 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:02,568 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:04,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:06,038 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:07,767 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:09,303 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:16,427 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:17,905 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:19,409 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:21,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:22,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:23,984 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:25,178 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:26,691 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:28,117 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:29,631 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:30,993 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:32,134 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:33,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:34,868 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:36,369 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:37,888 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:39,408 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:40,942 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:42,248 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:43,640 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:46,473 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:49,041 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:50,524 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:51,925 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:53,525 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:54,768 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:56,028 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:57,334 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:02:59,072 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:03:00,533 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:03:02,024 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:03:03,484 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:03:04,973 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:03:06,362 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph built and stored in Neo4j.\n"
     ]
    }
   ],
   "source": [
    "kg_docs = interaction_docs\n",
    "print(f\"KG docs from drug_interaction_docs: {len(kg_docs)}\")\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "neo4j_store = Neo4jGraphStore(\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    url=NEO4J_URI,\n",
    "    database=NEO4J_DATABASE,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=neo4j_store)\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    kg_docs,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=2,\n",
    ")\n",
    "\n",
    "print(\"Knowledge graph built and stored in Neo4j.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7761f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical_records=61, diet_records=13\n"
     ]
    }
   ],
   "source": [
    "splitter = SentenceSplitter(chunk_size=800, chunk_overlap=120)\n",
    "\n",
    "\n",
    "def build_records(docs: list, namespace: str) -> List[dict]:\n",
    "    nodes = splitter.get_nodes_from_documents(docs)\n",
    "    records: List[dict] = []\n",
    "\n",
    "    for i, node in enumerate(nodes):\n",
    "        content = node.get_content().strip()\n",
    "        if not content:\n",
    "            continue\n",
    "        meta = node.metadata or {}\n",
    "        source = meta.get(\"source\", \"unknown\")\n",
    "        citation = f\"{source}#chunk-{i}\"\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"_id\": f\"{namespace}_{i}\",\n",
    "                \"text\": content,  # must match Pinecone field_map text=content\n",
    "                \"source\": source,\n",
    "                \"doc_path\": meta.get(\"doc_path\", \"\"),\n",
    "                \"doc_tag\": meta.get(\"doc_tag\", \"\"),\n",
    "                \"citation\": citation,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "clinical_records = build_records(clinical_docs + interaction_docs, \"clinical_safety\")\n",
    "diet_records = build_records(diet_docs, \"dietician_docs\")\n",
    "\n",
    "print(f\"clinical_records={len(clinical_records)}, diet_records={len(diet_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a64b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert complete. Vectors are ready for RAG with citations.\n"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "if not pc.has_index(PINECONE_INDEX):\n",
    "    raise ValueError(\n",
    "        \"Pinecone index not found. Create it first with field_map text=content \"\n",
    "        \"and an integrated embedding model (e.g., llama-text-embed-v2).\"\n",
    "    )\n",
    "\n",
    "index = pc.Index(PINECONE_INDEX)\n",
    "\n",
    "\n",
    "def batch_upsert(index, namespace: str, records: List[dict], batch_size: int = 96) -> None:\n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch = records[i : i + batch_size]\n",
    "        if batch:\n",
    "            index.upsert_records(namespace, batch)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # Required wait for indexing before any search\n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "batch_upsert(index, \"clinical_safety\", clinical_records)\n",
    "batch_upsert(index, \"dietician_docs\", diet_records)\n",
    "\n",
    "print(\"Upsert complete. Vectors are ready for RAG with citations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41141bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'clinical_safety': {'vector_count': 61},\n",
      "                'dietician_docs': {'vector_count': 13}},\n",
      " 'total_vector_count': 74,\n",
      " 'vector_type': 'dense'}\n",
      "Namespaces: clinical_safety, dietician_docs\n"
     ]
    }
   ],
   "source": [
    "stats = index.describe_index_stats()\n",
    "print(stats)\n",
    "\n",
    "if stats.get(\"namespaces\"):\n",
    "    print(\"Namespaces:\", \", \".join(stats[\"namespaces\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26892a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RAG] namespace=clinical_safety query='What are key interactions or precautions for metformin?' top_k=3\n",
      "[RAG] hits=3\n",
      "  1. score=0.9011 cite=metformin_interactions.md#chunk-49 | # Metformin Drug Interactions  ## Drug Overview - **Generic Name:** Metformin - **Drug Class:** Biguanide - **Primary Use:** Type 2 Diabetes Mellitus (first-line therapy) - **Mecha...\n",
      "  2. score=0.7607 cite=metformin_interactions.md#chunk-54 | Liver Disease (Major) - **Conditions:** Hepatic impairment - **Risk:** Impaired lactate clearance - **Management:** Avoid in patients with clinical/laboratory evidence of hepatic d...\n",
      "  3. score=0.7558 cite=metformin_interactions.md#chunk-53 | Vancomycin, Trimethoprim, Digoxin, Procainamide, Quinidine, Morphine, Amiloride, Triamterene - **Interaction Type:** Pharmacokinetic competition - **Mechanism:** Compete for renal ...\n",
      "\n",
      "[RAG] namespace=dietician_docs query='Which hawker foods are good for diabetes?' top_k=3\n",
      "[RAG] hits=3\n",
      "  1. score=0.2451 cite=hawker_food_nutrition.md#chunk-0 | Source: doi: 10.3390/foods10071659   # Singaporean Hawker Food Nutritional Database *Data Source: Energy Content and Nutrient Profiles of Frequently Consumed Meals in Singapore (Pe...\n",
      "  2. score=0.0452 cite=hawker_food_nutrition.md#chunk-11 | white rice noodles, hard-boiled egg, dried bean curd in tangy gravy * **Portion**: 655g * **Nutritional Profile**:   * **Energy**: 694 kcal   * **Protein**: 28g   * **Fat**: 24g   ...\n",
      "  3. score=0.0412 cite=hawker_food_nutrition.md#chunk-10 | Low Fat, Low Carb, Low Calorie  ### Gado Gado * **Full Description**: Mixed vegetables with tofu, tempeh and peanut sauce * **Portion**: Per Serving * **Nutritional Profile**:   * ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 16:13:40,860 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 16:13:41,892 - WARNING - Index was not constructed with embeddings, skipping embedding usage...\n",
      "2026-02-03 16:13:43,173 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG] Response:\n",
      "The interactions involving insulin include the requirement for dose adjustments and an increase in hypoglycemia risk.\n"
     ]
    }
   ],
   "source": [
    "def search_with_citations(index, namespace: str, query: str, top_k: int = 5):\n",
    "    print(f\"\\n[RAG] namespace={namespace} query='{query}' top_k={top_k}\")\n",
    "    results = index.search(\n",
    "        namespace=namespace,\n",
    "        query={\n",
    "            \"top_k\": top_k * 2,\n",
    "            \"inputs\": {\"text\": query},\n",
    "        },\n",
    "        rerank={\n",
    "            \"model\": \"bge-reranker-v2-m3\",\n",
    "            \"top_n\": top_k,\n",
    "            \"rank_fields\": [\"text\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    hits = results.result.hits\n",
    "    print(f\"[RAG] hits={len(hits)}\")\n",
    "    for i, hit in enumerate(hits, start=1):\n",
    "        fields = hit.fields\n",
    "        citation = fields.get(\"citation\") or fields.get(\"source\", \"unknown\")\n",
    "        snippet = fields.get(\"text\", \"\")[:180].replace(\"\\n\", \" \")\n",
    "        score = hit[\"_score\"]\n",
    "        print(f\"  {i}. score={score:.4f} cite={citation} | {snippet}...\")\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "# RAG test queries (adjust as needed)\n",
    "_ = search_with_citations(\n",
    "    index,\n",
    "    \"clinical_safety\",\n",
    "    \"What are key interactions or precautions for metformin?\",\n",
    "    top_k=3,\n",
    ")\n",
    "_ = search_with_citations(\n",
    "    index,\n",
    "    \"dietician_docs\",\n",
    "    \"Which hawker foods are good for diabetes?\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "\n",
    "# Knowledge graph query test\n",
    "kg_engine = kg_index.as_query_engine(include_text=True, response_mode=\"tree_summarize\")\n",
    "kg_response = kg_engine.query(\"List interactions between insulin and food or other drugs.\")\n",
    "print(\"\\n[KG] Response:\")\n",
    "print(kg_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac460e-48d8-461e-9b7d-cc7b5c211b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc97675-a564-4c87-9c87-93f3b531707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449bdaa-ed11-41bd-80fe-70d0dbad06d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edee930-abdc-42e2-b262-b9f8f0b1712f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fa56c-983d-4650-b79a-414fa6e768e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
